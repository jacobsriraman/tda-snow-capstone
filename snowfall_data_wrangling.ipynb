{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>date</th>\n",
       "      <th>snow_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STILLWATER</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINERAL</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CARTER</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MADISON</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEERLODGE</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       County       date  snow_depth\n",
       "0  STILLWATER 2015-05-01         0.0\n",
       "1     MINERAL 2015-05-01         0.0\n",
       "2      CARTER 2015-05-01         0.0\n",
       "3     MADISON 2015-05-01         0.0\n",
       "4   DEERLODGE 2015-05-01         0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_snow_data(folder_path):\n",
    "    all_data = []\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read the first few rows to extract the year\n",
    "        with open(file_path, 'r') as file:\n",
    "            first_row = file.readlines()[0]  # Get the first row\n",
    "            year_month = first_row.split(',')[1].strip()  # Extract the month-year string\n",
    "            year = year_month.split(' ')[1]  # Split and take the year part\n",
    "\n",
    "        # Load the actual data, skipping the metadata rows\n",
    "        df = pd.read_csv(file_path, skiprows=4)\n",
    "\n",
    "        # Identify date columns by removing metadata-related columns\n",
    "        meta_cols = [\"GHCN ID\", \"Station Name\", \"County\", \"State\", \"Elevation\", \"Latitude\", \"Longitude\"]\n",
    "        date_cols = [col for col in df.columns if col not in meta_cols]\n",
    "\n",
    "        # Melt the dataframe to long format, keeping only date columns\n",
    "        df_long = df.melt(id_vars=[\"County\"], value_vars=date_cols, var_name=\"date\", value_name=\"snow_depth\")\n",
    "\n",
    "        # Add the year to the 'date' column\n",
    "        df_long['date'] = df_long['date'] + ' ' + year  # Append the extracted year to the date\n",
    "\n",
    "        # Replace 'M' with NaN and 'T' (trace amounts) with 0.01\n",
    "        df_long[\"snow_depth\"] = df_long[\"snow_depth\"].replace({\"M\": None, \"T\": 0.01}).astype(float)\n",
    "\n",
    "        # Append to the list\n",
    "        all_data.append(df_long)\n",
    "\n",
    "    # Concatenate all files\n",
    "    clean_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Convert 'date' to datetime\n",
    "    clean_data['date'] = pd.to_datetime(clean_data['date'], format='%b %d %Y')\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "# Example: preprocess data from a folder\n",
    "folder_path = '.\\data'\n",
    "cleaned_snow_data = preprocess_snow_data(folder_path)\n",
    "cleaned_snow_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  state_avg_snow\n",
      "0    2015-01-01        0.000386\n",
      "1    2015-01-02        0.175324\n",
      "2    2015-01-03        1.424501\n",
      "3    2015-01-04        1.132926\n",
      "4    2015-01-05        2.866150\n",
      "...         ...             ...\n",
      "3679 2025-01-27        0.000000\n",
      "3680 2025-01-28        0.000000\n",
      "3681 2025-01-29        0.000042\n",
      "3682 2025-01-30        0.018836\n",
      "3683 2025-01-31        0.036596\n",
      "\n",
      "[3684 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def aggregate_snow_data(snow_data, county_areas):\n",
    "    \n",
    "    # Step 1: Average sensor readings per county per day\n",
    "    county_avg_snow = snow_data.groupby([\"date\", \"County\"])[\"snow_depth\"].mean().reset_index()\n",
    "\n",
    "    # Step 2: Merge with county area data\n",
    "    county_avg_snow = county_avg_snow.merge(county_areas, on=\"County\", how=\"left\")\n",
    "\n",
    "    # Step 3: Compute the weighted average snow depth for the state\n",
    "    county_avg_snow[\"weighted_snow\"] = county_avg_snow[\"snow_depth\"] * county_avg_snow[\"AREA\"]\n",
    "    \n",
    "    # Compute the state-wide weighted average per day\n",
    "    state_snow = county_avg_snow.groupby(\"date\").apply(\n",
    "        lambda x: x[\"weighted_snow\"].sum() / x[\"AREA\"].sum()\n",
    "    ).reset_index(name=\"state_avg_snow\")\n",
    "    \n",
    "    # Ensure the result is in DataFrame format\n",
    "    return state_snow\n",
    "\n",
    "# Load county area data\n",
    "county_areas_df = pd.read_csv(\"MTcounties.csv\")\n",
    "\n",
    "# Example usage (assuming cleaned data is ready)\n",
    "aggregated_snow_data = aggregate_snow_data(cleaned_snow_data, county_areas_df)\n",
    "aggregated_snow_data = aggregated_snow_data.sort_values(by=\"date\")\n",
    "\n",
    "# Print or work with the DataFrame\n",
    "print(aggregated_snow_data)\n",
    "aggregated_snow_data.to_csv(\"cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the CSV files\n",
    "folder_path = \".\\montana_data_14_743_GoodStations\"\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = [\"STATION\", \"DATE\", \"NAME\", \"TMAX\", \"TMIN\", \"RHMN\", \"RHMX\"]\n",
    "\n",
    "# Define the date range\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "# List to store filtered dataframes\n",
    "filtered_dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):  # Process only CSV files\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read CSV while ensuring the DATE column is parsed as a datetime object\n",
    "        df = pd.read_csv(file_path, parse_dates=[\"DATE\"], dtype=str)\n",
    "\n",
    "        # Keep only the required columns (if they exist)\n",
    "        df = df[required_columns].copy()\n",
    "\n",
    "        # Filter rows within the date range\n",
    "        df = df[(df[\"DATE\"] >= start_date) & (df[\"DATE\"] <= end_date)]\n",
    "\n",
    "        # Append filtered dataframe to the list\n",
    "        filtered_dfs.append(df)\n",
    "\n",
    "# Concatenate all filtered dataframes into one (optional)\n",
    "final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "final_df['DATE'] = pd.to_datetime(final_df['DATE'], format = '%b %d %Y') # Make sure dates are datetime\n",
    "\n",
    "# Save to a new CSV file (optional)\n",
    "#final_df.to_csv(\"filtered_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping station names to county names\n",
    "station_to_county = {\n",
    "    'BOZEMAN GALLATIN FIELD AIRPORT, MT US': 'GALLATIN',\n",
    "    'GLASGOW INTERNATIONAL AIRPORT, MT US': 'VALLEY',\n",
    "    'BUTTE BERT MOONEY AIRPORT, MT US': 'SILVER BOW',\n",
    "    'CUT BANK AIRPORT, MT US': 'GLACIER',\n",
    "    'LIVINGSTON AIRPORT, MT US': 'PARK',\n",
    "    'HELENA AIRPORT ASOS, MT US': 'LEWISANDCLARK',\n",
    "    'BILLINGS INTERNATIONAL AIRPORT, MT US': 'YELLOWSTONE',\n",
    "    'MISSOULA INTERNATIONAL AIRPORT, MT US': 'MISSOULA',\n",
    "    'LEWISTOWN AIRPORT, MT US': 'FERGUS',\n",
    "    'GREAT FALLS AIRPORT, MT US': 'CASCADE',\n",
    "    'MILES CITY AIRPORT, MT US': 'CUSTER',\n",
    "    'KALISPELL GLACIER AIRPORT, MT US': 'FLATHEAD',\n",
    "    'DILLON AIRPORT, MT US': 'BEAVERHEAD',\n",
    "    'HAVRE AIRPORT ASOS, MT US': 'HILL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"COUNTY\"] = final_df[\"NAME\"].map(station_to_county)\n",
    "final_df.to_csv(\"filtered_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated weather data saved as time series for each variable.\n"
     ]
    }
   ],
   "source": [
    "def aggregate_weather_data(weather_data, county_areas):\n",
    "    # Step 1: Average weather readings per county per day\n",
    "    county_avg_weather = weather_data.groupby([\"DATE\", \"COUNTY\"])[[\"TMAX\", \"TMIN\", \"RHMN\", \"RHMX\"]].mean().reset_index()\n",
    "\n",
    "    # Step 2: Merge with county area data\n",
    "    county_avg_weather = county_avg_weather.merge(county_areas, on=\"COUNTY\", how=\"left\")\n",
    "\n",
    "    # Step 3: Compute weighted values\n",
    "    for col in [\"TMAX\", \"TMIN\", \"RHMN\", \"RHMX\"]:\n",
    "        county_avg_weather[f\"weighted_{col}\"] = county_avg_weather[col] * county_avg_weather[\"AREA\"]\n",
    "\n",
    "    # Step 4: Compute state-wide weighted averages per day\n",
    "    state_weather = county_avg_weather.groupby(\"DATE\").apply(\n",
    "        lambda x: pd.Series({col: x[f\"weighted_{col}\"].sum() / x[\"AREA\"].sum() for col in [\"TMAX\", \"TMIN\", \"RHMN\", \"RHMX\"]})\n",
    "    ).reset_index()\n",
    "\n",
    "    # Convert DATE to datetime format\n",
    "    state_weather[\"DATE\"] = pd.to_datetime(state_weather[\"DATE\"])\n",
    "\n",
    "    # Step 5: Create separate time series DataFrames for each weather variable\n",
    "    tmax_df = state_weather[[\"DATE\", \"TMAX\"]].rename(columns={\"TMAX\": \"State_Avg_TMAX\"})\n",
    "    tmin_df = state_weather[[\"DATE\", \"TMIN\"]].rename(columns={\"TMIN\": \"State_Avg_TMIN\"})\n",
    "    rhmn_df = state_weather[[\"DATE\", \"RHMN\"]].rename(columns={\"RHMN\": \"State_Avg_RHMN\"})\n",
    "    rhmx_df = state_weather[[\"DATE\", \"RHMX\"]].rename(columns={\"RHMX\": \"State_Avg_RHMX\"})\n",
    "\n",
    "    # Ensure full date range from 2015-01-01 to 2024-12-31\n",
    "    full_date_range = pd.date_range(start=\"2015-01-01\", end=\"2024-12-31\", freq=\"D\")\n",
    "    \n",
    "    def complete_timeseries(df, column_name):\n",
    "        df = df.set_index(\"DATE\").reindex(full_date_range).rename_axis(\"Date\").reset_index()\n",
    "        df = df.rename(columns={\"index\": \"Date\", column_name: column_name})\n",
    "        return df\n",
    "\n",
    "    tmax_df = complete_timeseries(tmax_df, \"State_Avg_TMAX\")\n",
    "    tmin_df = complete_timeseries(tmin_df, \"State_Avg_TMIN\")\n",
    "    rhmn_df = complete_timeseries(rhmn_df, \"State_Avg_RHMN\")\n",
    "    rhmx_df = complete_timeseries(rhmx_df, \"State_Avg_RHMX\")\n",
    "\n",
    "    return tmax_df, tmin_df, rhmn_df, rhmx_df\n",
    "\n",
    "# Load data\n",
    "weather_data = pd.read_csv(\"filtered_data.csv\")  # Your cleaned weather dataset with COUNTY column\n",
    "county_areas = pd.read_csv(\"MTcounties.csv\")  # County area dataset\n",
    "\n",
    "# Aggregate the data\n",
    "tmax_series, tmin_series, rhmn_series, rhmx_series = aggregate_weather_data(weather_data, county_areas)\n",
    "\n",
    "# Save to CSV\n",
    "tmax_series.to_csv(\"timeseries\\montana_tmax_timeseries.csv\", index=False)\n",
    "tmin_series.to_csv(\"timeseries\\montana_tmin_timeseries.csv\", index=False)\n",
    "rhmn_series.to_csv(\"timeseries\\montana_rhmn_timeseries.csv\", index=False)\n",
    "rhmx_series.to_csv(\"timeseries\\montana_rhmx_timeseries.csv\", index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Aggregated weather data saved as time series for each variable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
